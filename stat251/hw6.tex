\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\Pt}[1]{P(\text{#1})}
\nc{\nn}{\mathbb{N}}
\begin{document}
Name: Hall Liu

Date: \today 
\vspace{1.5cm}

\subsection*{1}
Starting at any time, the probability that the frog jumps left next is the probability that Process I occurs next before Process II. Since the next event that triggers is from Process I with probability $1/2$ (by symmetry of the processes -- relabeling I as II leaves the problem the same), the frog will jump left or right with equal probability. Denote the probability that the frog reaches $0$ from $k$ as $p_k$. Then, we have $p_k=\frac{1}{2}(p_{k-1}+p_{k+1})$ with $p_0=1$ and $p_n=0$. Since $p_k$ is the average of $p_{k-1}$ and $p_{k+1}$, we must have $p_{k+1}-p_k=p_k-p_{k-1}$ for all $k$, which means that the $p_k$ are given by $ak+b$ for some real $a,b$. Plugging in the endpoint values, we get $p_k=\frac{n-k}{k}$.
\subsection*{2}
Last week's hw showed that any ordering of a sequence of iid random variables is equally likely. We will use this result.

a. If $X_6>X_1$, that implies that $X_6>X_i$ for each $i<6$. Then, the probability of this is the number of configurations in which $X_6>X_1>X_i$ for $i=2,3,4,5$ divided by the number of configurations in which $X_5>X_i$ for $i=2,3,4,5$. The first is $4!$ by permuting the first $4$ elements in the ordering, and the second is $6\times4!=144$ by considering the $6$ possible positions of $X_6$ in the ordering, so the probability is $1/6$.

b. We want to count the number of configurations where $X_6>X_2$ and $X_1>X_i$ for $i=2,3,4,5$. If $X_6$ is the largest, there are $4!$ cases where this can be true. If $X_6$ is second-largest, there are still $4!$ cases where this can be true; if $X_6$ is third-largest, there are $3\cdots3!$ ways this can be true; fourth-largest is $2\times3!$; fifth-largest is $3!$, and smallest is $0$, for a total of $4!+4!+6\cdot3!=84$ ways, so the probability is $84/144=7/12$.
\subsection*{3}
Let $X=\mu+\sigma Z$, where $Z\sim N(0,1)$. Then, $E(e^{tX})=E(e^{t\mu+t\sigma Z})=E(e^{t\mu}e^{t\sigma Z})=e^{t\mu}E(e^{t\sigma Z})$. Integrate to find $E(e^{t\sigma Z})$.
\begin{align*}
    \int_{-\infty}^\infty e^{t\sigma z}\frac{1}{\sqrt{2\pi}}e^{\frac{-z^2}{2}}dz&=\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}e^{\frac{-z^2+2t\sigma z}{2}}dx\\
                                                                               &=\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}e^{-\frac{z^2-2t\sigma z+t^2\sigma^2}{2}+\frac{t^2\sigma^2}{2}}dx\\
                                                                               &=e^{\frac{t^2\sigma^2}{2}}\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}e^{\frac{-(z+t\sigma)^2}{2}}dx\\
                                                                               &=e^{\frac{t^2\sigma^2}{2}}
\end{align*}
Then, we have $E(e^{tX})=e^{t\mu+\frac{t^2\sigma^2}{2}}$.
\subsection*{4}
Follow the hint. We have $\sum Y_i\geq1\iff$ any of the $X_i$ are greater than $\sqrt{2\log n}+1$, which is equivalent to $M_n>\sqrt{2\log n}+1$. Also, $\sum Z_i=0\iff$ all $X_i$ are below $\sqrt{2\log n}-1$, which is equivalent to $M_n<\sqrt{2\log n}-1$ If we show that the probability that both these happen go to $0$, then we show that the probability that $M_n$ lies outside the range $(\sqrt{2\log n}-1,\sqrt{2\log n}+1)$ goes to $0$.

Using the Chernoff bounds from Section 8.5 (which don't depend on anything we haven't done yet, since we just derived the mgf for $N(0,1)$ above), we have:
$$P(X_i>\sqrt{2\log n}+1)\leq\exp(-t\sqrt{\log(n^2)}-t)e^{t^2/2}=\exp\left((t/\sqrt{2})^2-t(\sqrt{\log(n^2)}+1)+\left(\frac{\sqrt{\log(n^2)}+1}{\sqrt{2}}\right)^2-\left(\frac{\sqrt{\log(n^2)}+1}{\sqrt{2}}\right)^2\right)$$
Collecting the square term, we have $\exp((t/\sqrt{2}-(\sqrt{\log n}+1/\sqrt{2}))^2)$ multiplied by the leftover. Since the Chernoff bound holds for all $t>0$, we pick $t>0$ such that the difference in there is $0$, which means we can drop this term. Then, our upper bound on $P(X_i>\sqrt{2\log n}+1)$ becomes 
$$\exp(-\log n-\sqrt{\log(n^2)}-1/2)=\frac{e^{-\sqrt{\log(n^2)}}e^{-\frac{1}{2}}}{n}$$
Thus we have that
$$P(X_i<\sqrt{\log(n^2)}+1)\geq 1-\frac{e^{-\sqrt{\log(n^2)}}e^{-\frac{1}{2}}}{n}\implies P(\sum Y_i=0)\geq\left(1-\frac{e^{-\sqrt{\log(n^2)}}e^{-\frac{1}{2}}}{n}\right)^n$$
Since we have the numerator in the above expression going to $0$ and $\lim\limits_{x\to\infty}(1-a/x)^x=e^{-a}$, we have that this goes to $1$ as $n\to\infty$, which implies that $P(\sum Y_i\geq1)\to0$ as $n\to\infty$.

For the $Z$ case, I'll be referencing the upper bound on $Q(x)=1-\Phi(x)$ from \url{<http://en.wikipedia.org/wiki/Q-function#Bounds>}. We have $P(\sum Z_i=0)=(\Phi(\sqrt{\log n^2}-1)^n$ goes to $0$ as $n\to\infty$. Using the $Q$-bound, we have 
$$\Phi(\sqrt{\log n^2}-1)=1-Q(\sqrt{\log n^2}-1)\leq1-\frac{1}{\sqrt{2\pi}}\frac{\sqrt{\log n^2}-1}{\log n^2-2\sqrt{\log n^2}+2}\exp\left(-\frac{(\sqrt{\log n^2}-1)^2}{2}\right)$$
Expanding out the exponential term, we get $\frac{\exp(\sqrt{\log n^2}-1/2)}{n}$. Taking this whole thing to the $n$th power, we can put it in the form $\left(1-\frac{a}{n}\right)^n$ where 
$$a=\frac{1}{\sqrt{2\pi}}\frac{\sqrt{\log n^2}-1}{\log n^2-2\sqrt{\log n^2}+2}\exp(\sqrt{\log n^2}-1/2)$$
Note that $\exp(\sqrt{\log n^2})$ grows far faster than any other term in $a$ and therefore dominates. Since $a$ grows rapidly, we have that the overall expression goes to $0$ as $n\to\infty$, which is what we wanted in the first place.
\subsection*{5}
Not necessarily. Let $c(x,y)=2-2x-2y+4xy$. If $X,Y$ are jointly distributed according to $c$ on the unit square, then the marginal density of $X$ is $\int\limits_0^12-2x-2y+4xy dy=1$ and the same for $Y$. Consider the joint cdf $C(\Phi(z_1),\Phi(z_2))$ where $Z_1$ and $Z_2$ are standard normal RVs and $C(x,y)$ is the joint cdf corresponding to $c$. The marginal cdfs can be found by evaluating at $z_1=\infty$ or $z_2=\infty$, so the marginal cdf of $z_1$ is $C(\Phi(z_1),1)=\Phi(z_1)$, since $C$ has uniform marginals. Thus, the distribution with density $c(\Phi(z_1),\Phi(z_1))\phi(z_1)\phi(z_2)$ has standard normal marginals, but it's clearly not bivariate normal because $\Phi$ cannot be expressed in terms of elementary functions.
\subsection*{6}
Since $X_1$ and $X_2$ are distinguishable only up to labeling, $P(X_1<X_2)$ is invariant under swapping the labels, so we must have $P(x_1<X_2)=1/2$.
\subsection*{7}
Note that $P(X>\lambda)=1-P(X_1,X_2<\lambda)$ and the same for $Y$, so we just need to show that $P(Y_1,Y_2<\lambda)\geq P(X_1,X_2<\lambda)$. For $X$, independence implies that $P(X_1,X_2<\lambda)=P(X_1<\lambda)P(X_2<\lambda)=\Phi(\lambda)^2$. For $Y$, we have $P(Y_1,Y_2<\lambda)=P(Y_1<\lambda|Y_2<\lambda)P(Y_2<\lambda)$. From Example 5c in Chapter 6, we have that the conditional distribution of $Y_1$ given $Y_2=y$ is normal with mean $y/2$ and variance $3/4$. 

Now, we have $P(Y_1<\lambda|Y_2<\lambda)$ is the average value of $\Phi((\lambda-y)/\sqrt{3/4})$ over $Y_2<\lambda$. It's going to be greater than $\Phi((\lambda-0)/\sqrt{3/4})$ since averaging $Y_2$ over all of $\rn$ gives $0$, so we have that the conditional probability is greater than $\Phi(\lambda)=P(Y_1<\lambda)$, which means that $P(Y_1,Y_2<\lambda)>P(Y_1<\lambda)P(Y_2<\lambda)=P(X_1,X_2<\lambda)$.
%We have $P(X>\lambda)=P(X_1>\lambda\text{ or }X_2>\lambda)=P(X_1>\lambda)+P(X_2>\lambda)-P(X_1>\lambda\text{ and }X_2>\lambda)$ by inclusion-exclusion, and the same for $Y$. Expressing this in terms of the $Q$-function (defined in 4 above), we have $P(X>\lambda)=2Q(\lambda)-Q(\lambda)^2$, since $X_1$ and $X_2$ are independent.

%For $Y$, we can construct a bivariate joint density by using the covariance matrix $\Sigma=\openm1&1/2\\1/2&1\closem$ to get the density 
%$$f_{Y_1,Y_2}(y_1,y_2)=\frac{1}{2\pi\sqrt{3/4}}\exp\left(-\frac{2}{3}(y_1^2+y_2^2-y_1y_2)\right)$$
%so we have that $P(Y_1>\lambda\text{ and }Y_2>\lambda)=\int_\lambda^\infty\int_\lambda^\infty f_{Y_1,Y_2}(y_1,y_2)dy_1dy_2$
\end{document}
